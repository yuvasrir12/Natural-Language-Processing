{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further optimized by **Hyden J**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7SiCtre6pycE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "j5nx2guvsvWg"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "      <th>articleID</th>\n",
       "      <th>articleWordCount</th>\n",
       "      <th>byline</th>\n",
       "      <th>documentType</th>\n",
       "      <th>headline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>multimedia</th>\n",
       "      <th>newDesk</th>\n",
       "      <th>printPage</th>\n",
       "      <th>pubDate</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>snippet</th>\n",
       "      <th>source</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>webURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>58def1347c459f24986d7c80</td>\n",
       "      <td>716</td>\n",
       "      <td>By STEPHEN HILTNER and SUSAN LEHMAN</td>\n",
       "      <td>article</td>\n",
       "      <td>Finding an Expansive View  of a Forgotten Peop...</td>\n",
       "      <td>['Photography', 'New York Times', 'Niger', 'Fe...</td>\n",
       "      <td>3</td>\n",
       "      <td>Insider</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-04-01 00:15:41</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>One of the largest photo displays in Times his...</td>\n",
       "      <td>The New York Times</td>\n",
       "      <td>News</td>\n",
       "      <td>https://www.nytimes.com/2017/03/31/insider/nig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  abstract                 articleID  articleWordCount  \\\n",
       "0      NaN  58def1347c459f24986d7c80               716   \n",
       "\n",
       "                                byline documentType  \\\n",
       "0  By STEPHEN HILTNER and SUSAN LEHMAN      article   \n",
       "\n",
       "                                            headline  \\\n",
       "0  Finding an Expansive View  of a Forgotten Peop...   \n",
       "\n",
       "                                            keywords  multimedia  newDesk  \\\n",
       "0  ['Photography', 'New York Times', 'Niger', 'Fe...           3  Insider   \n",
       "\n",
       "   printPage              pubDate sectionName  \\\n",
       "0          2  2017-04-01 00:15:41     Unknown   \n",
       "\n",
       "                                             snippet              source  \\\n",
       "0  One of the largest photo displays in Times his...  The New York Times   \n",
       "\n",
       "  typeOfMaterial                                             webURL  \n",
       "0           News  https://www.nytimes.com/2017/03/31/insider/nig...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('ArticlesApril2017.csv')\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNXsjcuXszMG",
    "outputId": "17d8db58-25c3-46da-da19-547046031c85"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['finding an expansive view  of a forgotten people in niger',\n",
       "       'and now,  the dreaded trump curse',\n",
       "       'venezuela’s descent into dictatorship',\n",
       "       'stain permeates basketball blue blood',\n",
       "       'taking things for granted'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['headline'] = data['headline'].apply(str.lower)\n",
    "headlines = data['headline'].values\n",
    "\n",
    "headlines[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kQst5xoGp5fR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[180, 21, 682, 380, 4, 2, 683, 181, 5, 684],\n",
       " [6, 84, 1, 685, 11, 686],\n",
       " [687, 688, 134, 689],\n",
       " [690, 691, 692, 693, 694],\n",
       " [108, 182, 8, 695]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize text\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(headlines) # Create the vocabulary\n",
    "sequences = tokenizer.texts_to_sequences(headlines) # Use the vocabulary to convert text to seqs\n",
    "\n",
    "sequences[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "5yzSXnnJp7j-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[180, 21, 682, 380, 4, 2, 683, 181, 5],\n",
       "  [6, 84, 1, 685, 11],\n",
       "  [687, 688, 134]],\n",
       " [684, 686, 689])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare training data\n",
    "input_sequences = []\n",
    "X = []\n",
    "y = []\n",
    "for i in sequences:\n",
    "        X.append(i[:-1]) # Feature is everything except last element\n",
    "        y.append(i[-1]) # Target is last element\n",
    "  \n",
    "\n",
    "X[:3], y[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "oraFEqQvqCKQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 180,  21,\n",
       "        682, 380,   4,   2, 683, 181,   5]),\n",
       " 'LABEL',\n",
       " 684)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Padding: [23,45] to [0,0,23,45]\n",
    "X = pad_sequences(X) \n",
    "y = np.array(y)\n",
    "\n",
    "X[0],\"LABEL\",y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [  0],\n",
       "       [180],\n",
       "       [ 21],\n",
       "       [682],\n",
       "       [380],\n",
       "       [  4],\n",
       "       [  2],\n",
       "       [683],\n",
       "       [181],\n",
       "       [  5]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# LSTM accepts 3D input: No. of rows, No. of timesteps (words), No. of features per timestep (1 word so 1)\n",
    "lstm_shape = (X.shape[1],1) # to tell LSTM\n",
    "X_forLSTM = X.reshape(X.shape[0], X.shape[1], 1) # reshaping X shape\n",
    "\n",
    "X_forLSTM[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "qbjqmufgqDt0",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "b69e05f1-2fb2-4cf5-e609-148ae4b933ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ahmed\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# LSTM model\n",
    "model = Sequential([\n",
    "    LSTM(100, input_shape=lstm_shape),\n",
    "    Dense(vocab_size, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.01), \n",
    "    metrics=['accuracy'], \n",
    "    loss='sparse_categorical_crossentropy' # For multiple categories\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19aac13cbf0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_forLSTM, y, epochs=20, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "Ir3r2dutqIP6",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d8269fba-af19-4862-df53-020259dd595b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 257ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "\n",
      "Final generated text: The cat unknown name up soar crisis\n"
     ]
    }
   ],
   "source": [
    "num_words_to_generate = 5  # Generate 5 new words\n",
    "text = \"The cat\"\n",
    "\n",
    "for _ in range(num_words_to_generate):\n",
    "    # Turn the text into sequences [123,456,23]\n",
    "    token_list = tokenizer.texts_to_sequences([text])[0]\n",
    "\n",
    "    # Pad them [0,0,0,123,456,23]\n",
    "    padded_sequence = pad_sequences([token_list], maxlen=X.shape[1])\n",
    "\n",
    "    # Get the prediction (reshape: 1 row, X.shape[1] timesteps and 1 feature per timstep)\n",
    "    predicted = model.predict(padded_sequence.reshape(1, X.shape[1], 1))\n",
    "\n",
    "    # Find which word has this index\n",
    "    predicted_word = tokenizer.index_word[np.argmax(predicted)]\n",
    "    text += \" \" + predicted_word\n",
    "\n",
    "print(\"\\nFinal generated text:\", text)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
